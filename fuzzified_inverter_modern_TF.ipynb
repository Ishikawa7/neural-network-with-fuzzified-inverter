{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 12:46:39.982217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 12:46:42.184291: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 12:46:42.184419: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 12:46:42.184428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soggetto = \"soggetto_7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read soggetto_n_input.csv\n",
    "df_input = pd.read_csv(\"Datasets/\" + soggetto + '_input.csv', sep=',')\n",
    "# remove SIGMA and bit_type columns\n",
    "df_input = df_input.drop(['SIGMA', 'bit_type'], axis=1)\n",
    "#df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read soggetto_n_targets.csv\n",
    "df_target = pd.read_csv(\"Datasets/\" + soggetto + '_targets.csv', sep=',')\n",
    "#df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init_hidden = tf.keras.initializers.RandomUniform(\n",
    "    minval=-0.1, maxval=0.1, seed=101\n",
    ")\n",
    "w_init_output = tf.keras.initializers.RandomUniform(\n",
    "    minval=-0.1, maxval=0.1, seed=102\n",
    ")\n",
    "\n",
    "b_init_hidden = tf.keras.initializers.RandomUniform(\n",
    "    minval=-0.1, maxval=0.1, seed=103\n",
    ")\n",
    "b_init_output = tf.keras.initializers.RandomUniform(\n",
    "    minval=-0.1, maxval=0.1, seed=104\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perfect learning model\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(15, activation='tanh', input_shape=[len(df_input.keys())], kernel_initializer=w_init_hidden, bias_initializer=b_init_hidden),\n",
    "#     layers.Dense(15, activation='relu', kernel_initializer=w_init_output, bias_initializer=b_init_output),\n",
    "# ])\n",
    "# \n",
    "# # compile the model with the Adam optimizer and the mean squared error loss function\n",
    "# model.compile(optimizer= tf.optimizers.adam(learning_rate=0.001),\n",
    "#                 loss='mse',\n",
    "#                 metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 16:19:11.345028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-21 16:19:11.374010: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-21 16:19:11.374102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-429d08): /proc/driver/nvidia/version does not exist\n",
      "2023-02-21 16:19:11.375075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create neural network with an input layer, a hidden layer and an output layer respectively with 16, 15 and 15 neurons\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(15, activation='tanh', input_shape=[len(df_input.keys())], kernel_initializer=w_init_hidden, bias_initializer=b_init_hidden),\n",
    "    layers.Dense(15, activation='gelu', kernel_initializer=w_init_output, bias_initializer=b_init_output),\n",
    "])\n",
    "\n",
    "# compile the model with the Adam optimizer and the mean squared error loss function\n",
    "model.compile(optimizer= tf.optimizers.SGD(learning_rate=0.375, momentum=0.0),\n",
    "                loss='mse',\n",
    "                metrics=['mae', 'mse'],\n",
    "                run_eagerly = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with the input and target data\n",
    "history = model.fit(\n",
    "    df_input, df_target,\n",
    "    epochs=10000, verbose=0, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss       mae       mse  epoch\n",
       "9995  0.001041  0.005155  0.000058   9995\n",
       "9996  0.001009  0.005136  0.000056   9996\n",
       "9997  0.001043  0.005170  0.000058   9997\n",
       "9998  0.001012  0.005149  0.000056   9998\n",
       "9999  0.001046  0.005184  0.000058   9999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the loss function\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist['loss'] = hist['loss'] * df_input.shape[0]\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the loss function with seaborn\n",
    "sns.lineplot(x='epoch', y='loss', data=hist)\n",
    "# change figure size\n",
    "plt.rcParams['figure.figsize'] = [15, 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display model weights\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert weights to pandas dataframe\n",
    "df_weights_h = pd.DataFrame(weights[0])\n",
    "df_weights_o = pd.DataFrame(weights[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.236233</td>\n",
       "      <td>-0.782547</td>\n",
       "      <td>-4.985810</td>\n",
       "      <td>-4.489499</td>\n",
       "      <td>7.857164</td>\n",
       "      <td>-0.176759</td>\n",
       "      <td>-0.198209</td>\n",
       "      <td>-3.439954</td>\n",
       "      <td>-4.857010</td>\n",
       "      <td>2.974333</td>\n",
       "      <td>3.640291</td>\n",
       "      <td>2.240196</td>\n",
       "      <td>-4.745937</td>\n",
       "      <td>-0.840607</td>\n",
       "      <td>4.179140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.122036</td>\n",
       "      <td>0.176117</td>\n",
       "      <td>0.456491</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>-0.423206</td>\n",
       "      <td>0.089810</td>\n",
       "      <td>0.098017</td>\n",
       "      <td>0.463954</td>\n",
       "      <td>0.249908</td>\n",
       "      <td>-0.236994</td>\n",
       "      <td>-0.549253</td>\n",
       "      <td>-0.342572</td>\n",
       "      <td>0.370002</td>\n",
       "      <td>0.112509</td>\n",
       "      <td>0.051582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.129443</td>\n",
       "      <td>0.164646</td>\n",
       "      <td>0.407070</td>\n",
       "      <td>0.235377</td>\n",
       "      <td>-0.309978</td>\n",
       "      <td>0.095440</td>\n",
       "      <td>0.096659</td>\n",
       "      <td>0.624686</td>\n",
       "      <td>0.404605</td>\n",
       "      <td>-0.157746</td>\n",
       "      <td>-0.498946</td>\n",
       "      <td>-0.213769</td>\n",
       "      <td>0.404856</td>\n",
       "      <td>0.092245</td>\n",
       "      <td>0.009768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.064159</td>\n",
       "      <td>0.196486</td>\n",
       "      <td>0.745740</td>\n",
       "      <td>-0.540965</td>\n",
       "      <td>-1.476341</td>\n",
       "      <td>0.352092</td>\n",
       "      <td>0.205521</td>\n",
       "      <td>-0.180769</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.257212</td>\n",
       "      <td>0.167439</td>\n",
       "      <td>-0.147937</td>\n",
       "      <td>-0.910080</td>\n",
       "      <td>-0.167234</td>\n",
       "      <td>-0.338134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.910183</td>\n",
       "      <td>0.116028</td>\n",
       "      <td>-1.464432</td>\n",
       "      <td>-0.143774</td>\n",
       "      <td>0.483224</td>\n",
       "      <td>0.024093</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>-0.207543</td>\n",
       "      <td>1.535093</td>\n",
       "      <td>-1.198734</td>\n",
       "      <td>0.790196</td>\n",
       "      <td>-0.172021</td>\n",
       "      <td>1.541267</td>\n",
       "      <td>-0.035442</td>\n",
       "      <td>-0.872984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.089124</td>\n",
       "      <td>-0.024191</td>\n",
       "      <td>-0.069057</td>\n",
       "      <td>-0.056264</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.084293</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>0.093767</td>\n",
       "      <td>-0.070417</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>-0.098631</td>\n",
       "      <td>-0.067717</td>\n",
       "      <td>-0.022606</td>\n",
       "      <td>-0.097130</td>\n",
       "      <td>-0.016744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.301837</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>1.570949</td>\n",
       "      <td>0.143479</td>\n",
       "      <td>-0.525500</td>\n",
       "      <td>0.021541</td>\n",
       "      <td>0.174343</td>\n",
       "      <td>1.027206</td>\n",
       "      <td>-1.079608</td>\n",
       "      <td>0.651569</td>\n",
       "      <td>-1.013496</td>\n",
       "      <td>-0.361346</td>\n",
       "      <td>-0.999655</td>\n",
       "      <td>0.279364</td>\n",
       "      <td>0.987968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.043932</td>\n",
       "      <td>0.057394</td>\n",
       "      <td>0.357718</td>\n",
       "      <td>0.188951</td>\n",
       "      <td>-0.292239</td>\n",
       "      <td>-0.066481</td>\n",
       "      <td>0.119324</td>\n",
       "      <td>0.567213</td>\n",
       "      <td>0.399266</td>\n",
       "      <td>-0.215664</td>\n",
       "      <td>-0.515463</td>\n",
       "      <td>-0.396797</td>\n",
       "      <td>0.372170</td>\n",
       "      <td>0.188405</td>\n",
       "      <td>0.033063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.810338</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>-1.480042</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>0.642955</td>\n",
       "      <td>-0.096629</td>\n",
       "      <td>0.114715</td>\n",
       "      <td>-0.193785</td>\n",
       "      <td>1.546849</td>\n",
       "      <td>-1.307341</td>\n",
       "      <td>0.821532</td>\n",
       "      <td>-0.223404</td>\n",
       "      <td>1.502519</td>\n",
       "      <td>-0.157915</td>\n",
       "      <td>-0.838171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.947585</td>\n",
       "      <td>-0.093230</td>\n",
       "      <td>-0.562990</td>\n",
       "      <td>-0.136293</td>\n",
       "      <td>1.390363</td>\n",
       "      <td>-0.149239</td>\n",
       "      <td>-0.068517</td>\n",
       "      <td>0.255373</td>\n",
       "      <td>0.304235</td>\n",
       "      <td>0.133298</td>\n",
       "      <td>-0.036675</td>\n",
       "      <td>0.263184</td>\n",
       "      <td>1.388847</td>\n",
       "      <td>-0.147940</td>\n",
       "      <td>0.559634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.344331</td>\n",
       "      <td>-0.081358</td>\n",
       "      <td>-0.291790</td>\n",
       "      <td>-0.084849</td>\n",
       "      <td>0.970719</td>\n",
       "      <td>0.260070</td>\n",
       "      <td>0.490523</td>\n",
       "      <td>-0.510627</td>\n",
       "      <td>0.226116</td>\n",
       "      <td>0.154116</td>\n",
       "      <td>0.581089</td>\n",
       "      <td>1.203621</td>\n",
       "      <td>0.359469</td>\n",
       "      <td>0.039765</td>\n",
       "      <td>-0.221412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.045627</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.091764</td>\n",
       "      <td>0.052310</td>\n",
       "      <td>-0.076039</td>\n",
       "      <td>-0.080672</td>\n",
       "      <td>-0.087076</td>\n",
       "      <td>0.051286</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>-0.052343</td>\n",
       "      <td>0.038220</td>\n",
       "      <td>-0.082501</td>\n",
       "      <td>-0.059937</td>\n",
       "      <td>0.084795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.882064</td>\n",
       "      <td>-0.014039</td>\n",
       "      <td>-0.517717</td>\n",
       "      <td>-0.090896</td>\n",
       "      <td>1.468953</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-0.083269</td>\n",
       "      <td>0.360159</td>\n",
       "      <td>0.265655</td>\n",
       "      <td>0.031730</td>\n",
       "      <td>0.078628</td>\n",
       "      <td>0.198763</td>\n",
       "      <td>1.481168</td>\n",
       "      <td>-0.178867</td>\n",
       "      <td>0.473775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.984736</td>\n",
       "      <td>0.145265</td>\n",
       "      <td>0.361268</td>\n",
       "      <td>0.230752</td>\n",
       "      <td>-0.424658</td>\n",
       "      <td>-0.099850</td>\n",
       "      <td>-0.023288</td>\n",
       "      <td>0.644962</td>\n",
       "      <td>0.436807</td>\n",
       "      <td>-0.192601</td>\n",
       "      <td>-0.455146</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>0.446270</td>\n",
       "      <td>0.111444</td>\n",
       "      <td>0.090565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.050669</td>\n",
       "      <td>0.140216</td>\n",
       "      <td>0.758151</td>\n",
       "      <td>-0.419235</td>\n",
       "      <td>-1.399079</td>\n",
       "      <td>0.202046</td>\n",
       "      <td>0.124885</td>\n",
       "      <td>-0.250115</td>\n",
       "      <td>-0.212258</td>\n",
       "      <td>-0.175288</td>\n",
       "      <td>-0.003667</td>\n",
       "      <td>-0.172693</td>\n",
       "      <td>-0.911393</td>\n",
       "      <td>-0.246321</td>\n",
       "      <td>-0.372250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.533203</td>\n",
       "      <td>-0.026041</td>\n",
       "      <td>0.335759</td>\n",
       "      <td>0.885929</td>\n",
       "      <td>-0.037022</td>\n",
       "      <td>-0.337382</td>\n",
       "      <td>-0.369295</td>\n",
       "      <td>-0.930868</td>\n",
       "      <td>0.429512</td>\n",
       "      <td>0.603242</td>\n",
       "      <td>-0.362928</td>\n",
       "      <td>0.922729</td>\n",
       "      <td>0.334555</td>\n",
       "      <td>0.247442</td>\n",
       "      <td>-0.257760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -9.236233 -0.782547 -4.985810 -4.489499  7.857164 -0.176759 -0.198209   \n",
       "1   1.122036  0.176117  0.456491  0.324141 -0.423206  0.089810  0.098017   \n",
       "2   1.129443  0.164646  0.407070  0.235377 -0.309978  0.095440  0.096659   \n",
       "3  -1.064159  0.196486  0.745740 -0.540965 -1.476341  0.352092  0.205521   \n",
       "4   0.910183  0.116028 -1.464432 -0.143774  0.483224  0.024093  0.037976   \n",
       "5  -0.089124 -0.024191 -0.069057 -0.056264  0.059684  0.084293 -0.034215   \n",
       "6   0.301837  0.143069  1.570949  0.143479 -0.525500  0.021541  0.174343   \n",
       "7   1.043932  0.057394  0.357718  0.188951 -0.292239 -0.066481  0.119324   \n",
       "8   0.810338  0.005584 -1.480042 -0.089138  0.642955 -0.096629  0.114715   \n",
       "9   1.947585 -0.093230 -0.562990 -0.136293  1.390363 -0.149239 -0.068517   \n",
       "10 -0.344331 -0.081358 -0.291790 -0.084849  0.970719  0.260070  0.490523   \n",
       "11 -0.045627 -0.002474  0.007903  0.091764  0.052310 -0.076039 -0.080672   \n",
       "12  1.882064 -0.014039 -0.517717 -0.090896  1.468953 -0.166980 -0.083269   \n",
       "13  0.984736  0.145265  0.361268  0.230752 -0.424658 -0.099850 -0.023288   \n",
       "14 -1.050669  0.140216  0.758151 -0.419235 -1.399079  0.202046  0.124885   \n",
       "15  0.533203 -0.026041  0.335759  0.885929 -0.037022 -0.337382 -0.369295   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0  -3.439954 -4.857010  2.974333  3.640291  2.240196 -4.745937 -0.840607   \n",
       "1   0.463954  0.249908 -0.236994 -0.549253 -0.342572  0.370002  0.112509   \n",
       "2   0.624686  0.404605 -0.157746 -0.498946 -0.213769  0.404856  0.092245   \n",
       "3  -0.180769 -0.127254 -0.257212  0.167439 -0.147937 -0.910080 -0.167234   \n",
       "4  -0.207543  1.535093 -1.198734  0.790196 -0.172021  1.541267 -0.035442   \n",
       "5   0.093767 -0.070417 -0.008059 -0.098631 -0.067717 -0.022606 -0.097130   \n",
       "6   1.027206 -1.079608  0.651569 -1.013496 -0.361346 -0.999655  0.279364   \n",
       "7   0.567213  0.399266 -0.215664 -0.515463 -0.396797  0.372170  0.188405   \n",
       "8  -0.193785  1.546849 -1.307341  0.821532 -0.223404  1.502519 -0.157915   \n",
       "9   0.255373  0.304235  0.133298 -0.036675  0.263184  1.388847 -0.147940   \n",
       "10 -0.510627  0.226116  0.154116  0.581089  1.203621  0.359469  0.039765   \n",
       "11 -0.087076  0.051286  0.006873 -0.052343  0.038220 -0.082501 -0.059937   \n",
       "12  0.360159  0.265655  0.031730  0.078628  0.198763  1.481168 -0.178867   \n",
       "13  0.644962  0.436807 -0.192601 -0.455146 -0.278331  0.446270  0.111444   \n",
       "14 -0.250115 -0.212258 -0.175288 -0.003667 -0.172693 -0.911393 -0.246321   \n",
       "15 -0.930868  0.429512  0.603242 -0.362928  0.922729  0.334555  0.247442   \n",
       "\n",
       "          14  \n",
       "0   4.179140  \n",
       "1   0.051582  \n",
       "2   0.009768  \n",
       "3  -0.338134  \n",
       "4  -0.872984  \n",
       "5  -0.016744  \n",
       "6   0.987968  \n",
       "7   0.033063  \n",
       "8  -0.838171  \n",
       "9   0.559634  \n",
       "10 -0.221412  \n",
       "11  0.084795  \n",
       "12  0.473775  \n",
       "13  0.090565  \n",
       "14 -0.372250  \n",
       "15 -0.257760  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weights_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517002</td>\n",
       "      <td>0.512694</td>\n",
       "      <td>0.468814</td>\n",
       "      <td>0.751359</td>\n",
       "      <td>-0.248457</td>\n",
       "      <td>-0.680145</td>\n",
       "      <td>0.515139</td>\n",
       "      <td>0.747414</td>\n",
       "      <td>-0.364357</td>\n",
       "      <td>0.915059</td>\n",
       "      <td>-0.249626</td>\n",
       "      <td>-0.369420</td>\n",
       "      <td>0.512724</td>\n",
       "      <td>0.471714</td>\n",
       "      <td>0.305639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.131006</td>\n",
       "      <td>-0.125773</td>\n",
       "      <td>-0.075762</td>\n",
       "      <td>0.100662</td>\n",
       "      <td>-0.177602</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>-0.060776</td>\n",
       "      <td>0.126490</td>\n",
       "      <td>-0.054255</td>\n",
       "      <td>0.251437</td>\n",
       "      <td>-0.125366</td>\n",
       "      <td>0.012290</td>\n",
       "      <td>-0.061307</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.294166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.449487</td>\n",
       "      <td>0.450291</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>-0.481585</td>\n",
       "      <td>-0.579663</td>\n",
       "      <td>0.430181</td>\n",
       "      <td>0.446990</td>\n",
       "      <td>-0.479483</td>\n",
       "      <td>0.198643</td>\n",
       "      <td>0.264568</td>\n",
       "      <td>-0.573378</td>\n",
       "      <td>0.201405</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>0.122689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.135417</td>\n",
       "      <td>-0.135914</td>\n",
       "      <td>-0.929467</td>\n",
       "      <td>-0.014663</td>\n",
       "      <td>-1.724929</td>\n",
       "      <td>-0.077797</td>\n",
       "      <td>-0.134384</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>0.361245</td>\n",
       "      <td>0.060652</td>\n",
       "      <td>-1.689058</td>\n",
       "      <td>0.353889</td>\n",
       "      <td>-0.131527</td>\n",
       "      <td>-0.935086</td>\n",
       "      <td>0.414281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.415142</td>\n",
       "      <td>-0.421604</td>\n",
       "      <td>-0.858631</td>\n",
       "      <td>-0.296237</td>\n",
       "      <td>-1.168234</td>\n",
       "      <td>0.278011</td>\n",
       "      <td>-0.420033</td>\n",
       "      <td>-0.304074</td>\n",
       "      <td>1.150856</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>-1.190224</td>\n",
       "      <td>1.135914</td>\n",
       "      <td>-0.426980</td>\n",
       "      <td>-0.848534</td>\n",
       "      <td>-0.295506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.036859</td>\n",
       "      <td>-0.037586</td>\n",
       "      <td>0.061769</td>\n",
       "      <td>-0.213394</td>\n",
       "      <td>0.131086</td>\n",
       "      <td>0.293830</td>\n",
       "      <td>-0.041152</td>\n",
       "      <td>-0.242807</td>\n",
       "      <td>-0.021548</td>\n",
       "      <td>-0.041368</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>-0.141840</td>\n",
       "      <td>-0.062254</td>\n",
       "      <td>0.026636</td>\n",
       "      <td>0.144733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.152426</td>\n",
       "      <td>-0.143955</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>-0.170977</td>\n",
       "      <td>-0.112599</td>\n",
       "      <td>0.156240</td>\n",
       "      <td>-0.161850</td>\n",
       "      <td>-0.157432</td>\n",
       "      <td>-0.056180</td>\n",
       "      <td>0.284725</td>\n",
       "      <td>-0.050824</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>-0.145261</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>-0.234778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.140636</td>\n",
       "      <td>0.169790</td>\n",
       "      <td>0.145269</td>\n",
       "      <td>0.070788</td>\n",
       "      <td>-1.015649</td>\n",
       "      <td>-0.026274</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>0.067391</td>\n",
       "      <td>-0.044348</td>\n",
       "      <td>-0.204362</td>\n",
       "      <td>-1.025167</td>\n",
       "      <td>-0.086341</td>\n",
       "      <td>0.168210</td>\n",
       "      <td>0.116791</td>\n",
       "      <td>0.043631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.460896</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>-0.364418</td>\n",
       "      <td>-0.339180</td>\n",
       "      <td>-1.072082</td>\n",
       "      <td>0.477767</td>\n",
       "      <td>0.452048</td>\n",
       "      <td>-0.339317</td>\n",
       "      <td>0.403157</td>\n",
       "      <td>0.669632</td>\n",
       "      <td>-1.061099</td>\n",
       "      <td>0.408421</td>\n",
       "      <td>0.452001</td>\n",
       "      <td>-0.355563</td>\n",
       "      <td>0.145560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.264987</td>\n",
       "      <td>-0.296748</td>\n",
       "      <td>-0.208411</td>\n",
       "      <td>-0.309194</td>\n",
       "      <td>0.976536</td>\n",
       "      <td>0.384670</td>\n",
       "      <td>-0.283977</td>\n",
       "      <td>-0.301484</td>\n",
       "      <td>-0.141948</td>\n",
       "      <td>0.807478</td>\n",
       "      <td>0.987421</td>\n",
       "      <td>-0.110752</td>\n",
       "      <td>-0.281936</td>\n",
       "      <td>-0.191623</td>\n",
       "      <td>0.073415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.343293</td>\n",
       "      <td>-0.326023</td>\n",
       "      <td>-0.225983</td>\n",
       "      <td>-0.137193</td>\n",
       "      <td>0.946231</td>\n",
       "      <td>0.240342</td>\n",
       "      <td>-0.321136</td>\n",
       "      <td>-0.149609</td>\n",
       "      <td>-0.241500</td>\n",
       "      <td>0.457694</td>\n",
       "      <td>0.948815</td>\n",
       "      <td>-0.265631</td>\n",
       "      <td>-0.330993</td>\n",
       "      <td>-0.209264</td>\n",
       "      <td>0.070736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.198044</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>0.246417</td>\n",
       "      <td>0.780335</td>\n",
       "      <td>-0.324146</td>\n",
       "      <td>0.202163</td>\n",
       "      <td>0.248927</td>\n",
       "      <td>0.205236</td>\n",
       "      <td>-0.018416</td>\n",
       "      <td>0.830167</td>\n",
       "      <td>0.170689</td>\n",
       "      <td>0.204168</td>\n",
       "      <td>-0.241154</td>\n",
       "      <td>0.219087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.512516</td>\n",
       "      <td>0.520266</td>\n",
       "      <td>0.952373</td>\n",
       "      <td>-0.799794</td>\n",
       "      <td>-0.519293</td>\n",
       "      <td>0.707618</td>\n",
       "      <td>0.514182</td>\n",
       "      <td>-0.801022</td>\n",
       "      <td>-0.491836</td>\n",
       "      <td>-0.679783</td>\n",
       "      <td>-0.541243</td>\n",
       "      <td>-0.504735</td>\n",
       "      <td>0.513848</td>\n",
       "      <td>0.928932</td>\n",
       "      <td>-0.008977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.003637</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>0.214683</td>\n",
       "      <td>-0.082765</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.160037</td>\n",
       "      <td>-0.141588</td>\n",
       "      <td>0.174022</td>\n",
       "      <td>-0.105509</td>\n",
       "      <td>-0.198843</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>-0.132870</td>\n",
       "      <td>-0.295407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.268020</td>\n",
       "      <td>-0.258182</td>\n",
       "      <td>-0.110723</td>\n",
       "      <td>-0.364386</td>\n",
       "      <td>0.210053</td>\n",
       "      <td>0.547165</td>\n",
       "      <td>-0.255024</td>\n",
       "      <td>-0.372607</td>\n",
       "      <td>0.077105</td>\n",
       "      <td>-0.257389</td>\n",
       "      <td>0.200468</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>-0.262224</td>\n",
       "      <td>-0.112831</td>\n",
       "      <td>-0.074923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.517002  0.512694  0.468814  0.751359 -0.248457 -0.680145  0.515139   \n",
       "1  -0.131006 -0.125773 -0.075762  0.100662 -0.177602  0.025310 -0.060776   \n",
       "2   0.449487  0.450291 -0.003700 -0.481585 -0.579663  0.430181  0.446990   \n",
       "3  -0.135417 -0.135914 -0.929467 -0.014663 -1.724929 -0.077797 -0.134384   \n",
       "4  -0.415142 -0.421604 -0.858631 -0.296237 -1.168234  0.278011 -0.420033   \n",
       "5  -0.036859 -0.037586  0.061769 -0.213394  0.131086  0.293830 -0.041152   \n",
       "6  -0.152426 -0.143955 -0.008195 -0.170977 -0.112599  0.156240 -0.161850   \n",
       "7   0.140636  0.169790  0.145269  0.070788 -1.015649 -0.026274  0.169189   \n",
       "8   0.460896  0.452098 -0.364418 -0.339180 -1.072082  0.477767  0.452048   \n",
       "9  -0.264987 -0.296748 -0.208411 -0.309194  0.976536  0.384670 -0.283977   \n",
       "10 -0.343293 -0.326023 -0.225983 -0.137193  0.946231  0.240342 -0.321136   \n",
       "11  0.150376  0.198044 -0.204328  0.246417  0.780335 -0.324146  0.202163   \n",
       "12  0.512516  0.520266  0.952373 -0.799794 -0.519293  0.707618  0.514182   \n",
       "13 -0.003637  0.034818 -0.217918  0.214683 -0.082765  0.013720  0.018412   \n",
       "14 -0.268020 -0.258182 -0.110723 -0.364386  0.210053  0.547165 -0.255024   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.747414 -0.364357  0.915059 -0.249626 -0.369420  0.512724  0.471714   \n",
       "1   0.126490 -0.054255  0.251437 -0.125366  0.012290 -0.061307 -0.087614   \n",
       "2  -0.479483  0.198643  0.264568 -0.573378  0.201405  0.450145 -0.008025   \n",
       "3  -0.012073  0.361245  0.060652 -1.689058  0.353889 -0.131527 -0.935086   \n",
       "4  -0.304074  1.150856 -0.045166 -1.190224  1.135914 -0.426980 -0.848534   \n",
       "5  -0.242807 -0.021548 -0.041368  0.040046 -0.141840 -0.062254  0.026636   \n",
       "6  -0.157432 -0.056180  0.284725 -0.050824  0.002409 -0.145261  0.016432   \n",
       "7   0.067391 -0.044348 -0.204362 -1.025167 -0.086341  0.168210  0.116791   \n",
       "8  -0.339317  0.403157  0.669632 -1.061099  0.408421  0.452001 -0.355563   \n",
       "9  -0.301484 -0.141948  0.807478  0.987421 -0.110752 -0.281936 -0.191623   \n",
       "10 -0.149609 -0.241500  0.457694  0.948815 -0.265631 -0.330993 -0.209264   \n",
       "11  0.248927  0.205236 -0.018416  0.830167  0.170689  0.204168 -0.241154   \n",
       "12 -0.801022 -0.491836 -0.679783 -0.541243 -0.504735  0.513848  0.928932   \n",
       "13  0.160037 -0.141588  0.174022 -0.105509 -0.198843 -0.021429 -0.132870   \n",
       "14 -0.372607  0.077105 -0.257389  0.200468  0.055216 -0.262224 -0.112831   \n",
       "\n",
       "          14  \n",
       "0   0.305639  \n",
       "1   0.294166  \n",
       "2   0.122689  \n",
       "3   0.414281  \n",
       "4  -0.295506  \n",
       "5   0.144733  \n",
       "6  -0.234778  \n",
       "7   0.043631  \n",
       "8   0.145560  \n",
       "9   0.073415  \n",
       "10  0.070736  \n",
       "11  0.219087  \n",
       "12 -0.008977  \n",
       "13 -0.295407  \n",
       "14 -0.074923  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weights_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the two dataframes to csv files\n",
    "# df_weights_h.to_csv(\"Datasets/\" + soggetto + '_weights_h.csv', index=False)\n",
    "# df_weights_o.to_csv(\"Datasets/\" + soggetto + '_weights_o.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cf3148ee2412bf818af45c9d8f3260092e813536e8161d4ddb61a7fba27a160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
